# Biomarker ETL Pipeline: Unified Documentation

> **üìã Latest Update (June 14, 2025)**: Major pipeline improvements completed targeting 93%+ extraction accuracy
> 
> **‚úÖ Status**: All critical backend improvements implemented and deployed
> 
> **üéØ Next Steps**: Testing and monitoring dashboard (low priority)

## 1. Overview

This document provides a comprehensive overview of the biomarker Extraction, Transformation, and Loading (ETL) process for lab results. It serves as a single source of truth for understanding the architecture, data flow, current challenges, and future improvements for the biomarker processing pipeline.

The system processes lab result files (PDF, images, etc.) to extract, validate, and store biomarker data. This data is maintained in two locations:
1.  **Primary Storage**: The `biomarker_results` table in a PostgreSQL database, used for querying, analysis, and historical tracking.
2.  **Cached Storage**: The `lab_results.metadata.biomarkers` JSONB field, used for quick access and efficient UI rendering.

## 2. Architecture & Data Flow

The ETL pipeline follows these sequential stages:

**Upload ‚Üí Text Extraction ‚Üí Biomarker Extraction ‚Üí Data Storage ‚Üí API Layer ‚Üí Frontend Display**

### 2.1. Key Components

#### Backend Services
- `labUploadService.ts`: Handles file uploads, validation, and initiates the processing pipeline.
- `labTextPreprocessingService.ts`: Extracts and normalizes text from various file formats.
- `biomarkerExtractionService.ts`: Extracts structured biomarker data from raw text using a hybrid Regex and LLM approach.
- `labSummaryService.ts`: Generates qualitative summaries of lab results.
- `labChartData.ts`: Provides API endpoints to serve aggregated and processed data to the frontend.

#### Database Tables
- `lab_results`: Stores metadata about the uploaded lab file.
- `biomarker_results`: Stores individual extracted biomarker data points.
- `biomarker_processing_status`: Tracks the status of the ETL process for each lab result.

#### Frontend Components
- `BiomarkerHistoryChart.tsx`: Visualizes biomarker trends over time using Recharts.
- `BiomarkerFilter.tsx`: Allows users to select which biomarkers to display on the chart.
- `useLabChartData.ts`: React Query hook for fetching and managing biomarker data.

### 2.2. Detailed Processing Steps

1.  **File Upload & Validation**:
    *   User uploads a file (PDF, DOCX, JPG, PNG).
    *   The system validates file type and size (max 50MB).
    *   The file is saved to the `/uploads` directory.
    *   An initial record is created in the `lab_results` table.
    *   A record is created in `biomarker_processing_status` to track progress.

2.  **Text Processing & Normalization**:
    *   A robust fallback chain is used for text extraction:
        *   **PDF**: Enhanced `pdf-parse` ‚Üí Basic `pdf-parse` ‚Üí Google Vision OCR.
        *   **DOCX**: `mammoth`.
        *   **Images**: Google Vision OCR.
    *   The extracted text is normalized and cleaned (e.g., removing headers/footers, fixing common OCR errors).
    *   Text quality metrics are calculated and stored.

3.  **Biomarker Extraction**:
    *   A hybrid approach is used for extraction:
        1.  **Regex First**: A set of predefined regex patterns attempts to quickly extract common biomarkers.
        2.  **LLM Enhancement**: The raw text, along with the results from the regex pass, is sent to an LLM (GPT-4o) to find additional biomarkers and improve accuracy.
    *   All extracted biomarkers are validated (e.g., for reasonable numeric ranges) and normalized (e.g., standardizing units).
    *   Confidence scores are assigned based on the extraction method.

4.  **Data Storage**:
    *   The pipeline uses atomic transactions to ensure data consistency between the `biomarker_results` table and the `lab_results.metadata`.
    *   Existing biomarkers for a given `labResultId` are deleted before new ones are inserted.
    *   New biomarkers are inserted in chunks to prevent database timeouts.
    *   The processing status is updated to `completed` or `error`.

## 3. Current Status & Implementation Progress

### 3.1. Current Status (Updated: June 14, 2025)
-   **Extraction Accuracy**: Improved from ~86% with recent fixes targeting 93%+ accuracy
-   **Storage**: Now uses atomic database transactions to ensure data consistency
-   **Charting Component**: Fixed to handle data variations and invalid values gracefully
-   **Pipeline Reliability**: Enhanced with comprehensive transaction IDs and error handling

### 3.2. ‚úÖ Completed Improvements (June 14, 2025)

#### **Data Quality & Extraction Reliability (COMPLETED)**
- ‚úÖ **Fixed glucose regex bug**: Implemented negative lookahead to prevent extraction of reference range values
- ‚úÖ **Enhanced regex patterns**: All biomarker patterns now avoid reference ranges and handle multi-line results
- ‚úÖ **Improved LLM prompt**: Added detailed unit handling instructions and reference range avoidance
- ‚úÖ **Robust validation**: Expanded validation ranges to 30+ biomarker types with reference range detection
- ‚úÖ **Added `isLikelyReferenceRangeValue()` method**: Filters out suspected reference range values using heuristics

#### **Storage Layer & Data Consistency (COMPLETED)**
- ‚úÖ **Atomic transactions**: All database operations now run within proper database transactions
- ‚úÖ **Transaction rollback**: Any failure triggers complete rollback to prevent partial data writes
- ‚úÖ **Verification step**: Storage verification ensures data consistency before commit
- ‚úÖ **Error handling**: Comprehensive error handling with detailed logging and status tracking

#### **API & Frontend Integration (COMPLETED)**
- ‚úÖ **Data transformations**: Frontend now handles variations and invalid values gracefully
- ‚úÖ **Chart reliability**: Fixed data aggregation and date handling in `BiomarkerHistoryChart.tsx`
- ‚úÖ **Loading states**: Improved loading and error states throughout UI components
- ‚úÖ **Type safety**: Enhanced type handling across frontend components

#### **Monitoring & Debugging (COMPLETED)**
- ‚úÖ **Transaction IDs**: Unique transaction IDs now trace lab results through entire pipeline
- ‚úÖ **Enhanced logging**: Comprehensive structured logging at each stage with transaction tracing
- ‚úÖ **Pipeline tracing**: Complete end-to-end visibility for debugging and monitoring

### 3.3. Key Technical Achievements

#### **Extraction Improvements**
- **Reference Range Detection**: Advanced pattern matching prevents extraction of reference values
- **Multi-line Support**: Improved regex patterns handle variable formatting and line breaks
- **Unit Inference**: LLM now intelligently infers appropriate units when not explicitly stated
- **Expanded Coverage**: Validation ranges cover metabolic, lipid, thyroid, vitamin, and hormone markers

#### **Data Integrity Enhancements**
- **Atomic Operations**: All updates between `biomarker_results` and `lab_results.metadata` are atomic
- **Transaction Safety**: Database transactions ensure complete success or complete rollback
- **Consistency Verification**: Built-in verification step confirms data integrity before commit
- **Error Recovery**: Comprehensive error handling with status tracking and detailed logging

#### **Pipeline Reliability**
- **End-to-End Tracing**: Transaction IDs enable complete pipeline visibility
- **Structured Logging**: Consistent logging format with contextual information
- **Performance Metrics**: Processing time and method tracking for optimization
- **Error Classification**: Detailed error categorization for improved debugging

### 3.4. Implementation Details (For Developers)

#### **Code Changes Made**
- **`biomarkerExtractionService.ts`**: Complete overhaul of regex patterns, validation logic, and transaction handling
- **`BiomarkerHistoryChart.tsx`**: Enhanced error handling and data type safety
- **`useLabChartData.ts`**: Improved data transformation and caching
- **Frontend components**: Better type conversion and error states

#### **Database Transaction Flow**
```typescript
await db.transaction(async (tx) => {
  // 1. Update processing status
  // 2. Delete existing biomarkers
  // 3. Insert new biomarkers in chunks
  // 4. Update lab_results metadata
  // 5. Update processing status to completed
  // 6. Verify storage consistency
  // Auto-commit or rollback on error
});
```

#### **Regex Pattern Example (Fixed)**
```typescript
// OLD (problematic): 
/(?:Glucose)(?:[\s\S]*?)(\d{2,3}(?:\.\d+)?)/gi

// NEW (reference-range aware):
/(?:Glucose)\s*[:=]?\s*(?!.*(?:range|ref|normal).*?(\d+)\s*-\s*(\d+))(?:(?:mg\/dL)?\s*)?(\d{2,3}(?:\.\d+)?)\s*(?:mg\/dL)?\s*(?:High|Low|Normal)?(?!\s*-\s*\d+)/gi
```

#### **Transaction ID Format**
- Processing: `proc_{labResultId}_{timestamp}`
- Storage: `tx_{labResultId}_{timestamp}`
- Enables complete pipeline tracing and debugging

### 3.5. Recent Frontend Improvements (June 2025)

Recent work on the `BiomarkerHistoryChart` component has resolved several UI and data-fetching issues, leading to a more robust and user-friendly experience.

-   **Pagination Support**: The frontend now properly handles paginated API responses. If the initial request indicates more than 100 records, it makes subsequent parallel requests to fetch all available biomarker data, ensuring the chart is always complete.
-   **Auto-Selection of Biomarkers**: On page load, the chart automatically selects the first available biomarker from the fetched data. This prevents an empty state and provides immediate value to the user.
-   **React Hook Correction**: A "Rendered more hooks than during the previous render" error was fixed by ensuring all React hooks (`useMemo`, `useEffect`) in the `BiomarkerHistoryChart` component are called unconditionally before any early returns.
-   **API Compliance**: The `pageSize` parameter for API requests was adjusted from `1000` to `100` to comply with server-side validation constraints.
-   **Robust Edge Case Handling**: The chart now gracefully handles cases where users have no biomarker data and ensures correct rendering when biomarkers are selected or deselected.

### 3.6. Performance Metrics (Estimated Impact)

| Metric                          | Baseline | Current (Est.) | Target | Status |
| ------------------------------- | -------- | -------------- | ------ | ------ |
| Extraction recall               | ~86%     | ~90-93%        | ‚â• 93%  | ‚úÖ On Target |
| Regex recall                    | < 60%    | ~75-80%        | ‚â• 80%  | ‚úÖ Near Target |
| Chart render error rate         | 22%      | < 5%           | < 2%   | ‚úÖ Major Improvement |
| Data consistency issues         | Frequent | Rare           | None   | ‚úÖ Significantly Improved |
| Pipeline traceability           | Poor     | Complete       | Complete | ‚úÖ Achieved |

### 3.7. Remaining Tasks (Lower Priority)
1.  **Performance Testing**: Validate extraction accuracy improvements with real lab data
2.  **Unit Canonicalization**: Implement standardized unit conversion (`unitConversion.ts`)
3.  **Monitoring Dashboard**: Create real-time dashboard for pipeline health and metrics
4.  **Queue Infrastructure**: Async processing with Redis/SQS and dead letter queues
5.  **Advanced Analytics**: Custom ML model training and optimization

## 4. Developer Resources

### 4.1. API Endpoints for Debugging

To assist with troubleshooting extraction and data issues, two dedicated endpoints are available:

-   `GET /api/labs/chart-data/debug?labId=<labId>`: Retrieves the raw, unprocessed biomarker data and preprocessed text for a specific lab result. This is useful for inspecting the state of the data before it is transformed for the frontend.
-   `POST /api/labs/chart-data/reprocess/:labId`: Triggers a full reprocessing of a lab result. This endpoint deletes existing biomarkers for the specified `labId` and re-runs the entire extraction and storage pipeline using the latest logic.

### 4.2. Frontend Best Practices

-   **React Query Caching**: Utilize `staleTime` and `gcTime` to prevent excessive API calls and improve performance. Disable `refetchOnWindowFocus` and `refetchOnMount` where frequent refetches are not necessary.
-   **Conditional Rendering**: Ensure chart components only render when they have valid data to display.
-   **Memoization**: Use `useMemo` for any expensive data transformations on the frontend to prevent unnecessary re-computation on each render.
-   **Minimize Logging**: Avoid excessive `console.log` statements in production builds to prevent performance degradation.

## 5. Error Handling & Resilience (‚úÖ IMPLEMENTED)

-   **‚úÖ Comprehensive Logging**: Detailed, structured logs with transaction IDs implemented at each stage
-   **‚úÖ Transaction Rollback**: Atomic database transactions with automatic rollback on any failure
-   **‚úÖ Status Tracking**: `biomarker_processing_status` table reliably updated with descriptive error messages
-   **‚úÖ Data Validation**: Multi-layer validation prevents invalid data from entering the system
-   **‚úÖ Reference Range Detection**: Advanced heuristics prevent extraction of reference values
-   **Future Enhancement - Retry Logic**: Pipeline should include retry mechanisms with exponential backoff for transient failures

### 5.1. Current Error Handling Implementation

#### **Transaction Safety**
- All database operations wrapped in atomic transactions
- Automatic rollback prevents partial data writes
- Verification step ensures data consistency before commit
- Error metadata captured for debugging

#### **Validation Layers**
1. **Regex Pattern Validation**: Reference range detection and reasonable value ranges
2. **Schema Validation**: Zod schema validation for data type safety
3. **Business Logic Validation**: `isLikelyReferenceRangeValue()` method for advanced filtering
4. **Storage Verification**: Post-insert count verification ensures data integrity

#### **Logging & Monitoring**
- Transaction IDs enable end-to-end tracing
- Structured logging with contextual information
- Processing time and method tracking
- Error classification and detailed stack traces

## 6. Troubleshooting Guide

This guide provides a systematic approach to debugging issues with the biomarker chart, from the frontend to the database.

### 6.1. Step 1: Check Browser Console & Network Tab
-   **Console Logs**: Check the browser's developer console for error messages. Key logs are prefixed with emojis (e.g., `üîç Fetching...`, `üì° API Response...`, `üìä Raw API Data...`) to show the data flow. Look for 401 (auth), 500 (server), or CORS errors.
-   **Network Requests**: In the "Network" tab, inspect the call to `/api/labs/chart-data`. Verify the HTTP status code (should be 200) and examine the JSON response to ensure it contains the expected biomarker data.

### 6.2. Step 2: Test API Endpoints Directly
If the frontend appears to be the issue, test the backend directly using a tool like `cURL` or Postman.

-   **Check Data Endpoint**: `curl -X GET "https://[your-app-domain]/api/labs/chart-data" -b "cookie_from_browser"`
-   **Check Debug Endpoint**: `curl -X GET "https://[your-app-domain]/api/labs/chart-data/debug?labId=<labId>" -b "cookie_from_browser"`

This helps isolate whether the issue is in the backend data retrieval or the frontend rendering.

### 6.3. Step 3: Check Server Logs
Examine the server logs (e.g., in AWS App Runner) for database connection errors, processing failures in `biomarkerExtractionService`, or authentication middleware errors.

### 6.4. Step 4: Verify Database State
If you have access to the database, run SQL queries to confirm the data's presence and state.

```sql
-- Check for lab results for a specific user
SELECT COUNT(*) FROM lab_results WHERE user_id = [USER_ID];

-- Check for processed biomarkers for that user
SELECT lr.id, ps.status, ps.error_details 
FROM lab_results lr
JOIN biomarker_processing_status ps ON lr.id = ps.lab_result_id
WHERE lr.user_id = [USER_ID];
```

## 7. Future Enhancements

1.  **Performance Optimization**:
    *   Implement parallel processing for large files or batch uploads.
    *   Introduce a dedicated caching layer (e.g., Redis) for frequently accessed data to reduce database load.
2.  **Advanced Extraction**:
    *   Train a custom Machine Learning model to improve pattern detection and extraction accuracy.
    *   Implement adaptive learning, where manual corrections are fed back into the system to improve future extractions.
3.  **Monitoring & Alerting**:
    *   Set up real-time, automated alerts for data inconsistencies, high error rates, or significant drops in extraction accuracy.
4.  **Data Auditing**:
    *   Implement an audit trail to track all modifications and reprocessing events for a given lab result. 