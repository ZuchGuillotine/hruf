// Updated version of queryWithAI in openaiQueryService.ts

export async function* queryWithAI(messages: Array<{ role: string; content: string }>, userId: string | null) {
  try {
    // Get the last user message which contains the query
    const userQuery = messages[messages.length - 1].content;

    // Build context using the context service
    const userIdNum = userId ? parseInt(userId) : null;
    const context = await constructQueryContext(userIdNum, userQuery);

    // Debug log the context only once before processing
    await debugContext(userId || 'anonymous', context, 'query');

    // Create a unique request ID for logging
    const requestId = `query_${Date.now()}_${Math.random().toString(36).substring(2, 15)}`;
    
    // Log processing details for debugging
    console.log('Processing query with OpenAI:', {
      requestId,
      userId,
      userIdType: typeof userId,
      messageCount: context.messages.length,
      isAuthenticated: !!userId,
      model: MODELS.QUERY_CHAT,
      timestamp: new Date().toISOString()
    });

    try {
      // Call OpenAI API with chat completion
      // IMPORTANT: This service must always use the QUERY_CHAT model (o3-mini)
      console.log('Using query chat model:', {
        requestId,
        model: MODELS.QUERY_CHAT,
        modelName: "o3-mini",
        isAuthenticated: !!userId,
        timestamp: new Date().toISOString()
      });

      const stream = await openai.chat.completions.create({
        model: MODELS.QUERY_CHAT,
        messages: context.messages,
        max_completion_tokens: 1000,
        stream: true
      });

      let fullResponse = "";

      // Process each chunk from the stream
      for await (const chunk of stream) {
        const content = chunk.choices[0]?.delta?.content || "";

        if (content) {
          console.log('Processing stream chunk:', {
            requestId,
            contentLength: content.length,
            preview: content.substring(0, 30),
            timestamp: new Date().toISOString()
          });

          fullResponse += content;
          yield { response: content, streaming: true };
        }
      }

      // If user is authenticated, save to query_chats table instead of qualitative_logs
      if (userId) {
        try {
          // Import the query_chats schema if it exists
          const { queryChats } = await import('../../db/schema');
          
          // Save to query_chats if available, otherwise use qualitative_logs with query type
          if (queryChats) {
            await db
              .insert(queryChats)
              .values({
                userId: parseInt(userId),
                messages: JSON.stringify([
                  { role: 'user', content: userQuery },
                  { role: 'assistant', content: fullResponse }
                ]),
                loggedAt: new Date(),
                metadata: {
                  savedAt: new Date().toISOString(),
                  queryType: 'supplement_info',
                  model: MODELS.QUERY_CHAT
                }
              });
              
            console.log('Saved query chat to queryChats table:', {
              requestId,
              userId,
              timestamp: new Date().toISOString()
            });
          } else {
            // Fallback to qualitative_logs with explicit query type
            await db
              .insert(qualitativeLogs)
              .values({
                userId: parseInt(userId),
                content: `Query: ${userQuery}\n\nResponse: ${fullResponse}`,
                type: 'query', // Important: mark as query
                tags: ['ai_query'],
                metadata: {
                  savedAt: new Date().toISOString(),
                  queryType: 'supplement_info',
                  model: MODELS.QUERY_CHAT
                }
              });
              
            console.log('Saved query chat to qualitativeLogs table with type=query:', {
              requestId,
              userId,
              timestamp: new Date().toISOString()
            });
          }
        } catch (saveError) {
          console.error("Failed to save query interaction:", {
            requestId,
            error: saveError instanceof Error ? saveError.message : String(saveError),
            stack: saveError instanceof Error ? saveError.stack : undefined,
            timestamp: new Date().toISOString()
          });
          // Continue anyway since response was already sent
        }
      }

      // Log successful completion
      console.log('OpenAI query completed successfully:', {
        requestId,
        userId,
        responseLength: fullResponse.length,
        timestamp: new Date().toISOString()
      });

      // Send final chunk
      yield { response: "", streaming: false };

    } catch (streamError) {
      console.error("OpenAI query stream error:", {
        requestId,
        error: streamError instanceof Error ? streamError.message : 'Unknown error',
        stack: streamError instanceof Error ? streamError.stack : undefined,
        userId,
        timestamp: new Date().toISOString()
      });
      
      // Return error to client
      yield { 
        error: streamError instanceof Error ? streamError.message : "Error connecting to AI service", 
        streaming: false 
      };
    }
  } catch (error) {
    console.error("OpenAI query error:", {
      error: error instanceof Error ? error.message : 'Unknown error',
      stack: error instanceof Error ? error.stack : undefined,
      userId,
      timestamp: new Date().toISOString()
    });
    
    // Return friendly error message
    yield { 
      error: "I'm having trouble understanding your question right now. Please try again in a moment.",
      streaming: false 
    };
  }
}