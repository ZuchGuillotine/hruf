// server/openai.ts

import OpenAI from "openai";
import logger from "./utils/logger";

// Ensure API key is present
if (!process.env.OPENAI_API_KEY) {
  throw new Error("OPENAI_API_KEY must be set");
}

// Initialize OpenAI client
const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

/**
 * System prompt that defines the AI assistant's behavior and capabilities
 */
export const SYSTEM_PROMPT = `You are a friendly and insightful assistant designed to help users reflect on their supplement regimen and share qualitative feedback about their experiences. Your role is to engage the user with thoughtful follow-up questions and encourage detailed responses about how specific supplements are affecting their mood, energy, and overall well-being.`;

/**
 * Main function to interact with OpenAI's chat API
 * @param messages - Array of message objects containing role and content
 * @returns AsyncGenerator yielding streaming chunks of the AI's response
 */
export async function* chatWithAI(messages: Array<{ role: string; content: string }>) {
  try {
    // Calculate token usage for logging purposes
    const estimatedTokenCount = messages.reduce((total, msg) => {
      // Rough estimation: ~4 characters per token
      return total + Math.ceil(msg.content.length / 4);
    }, 0);
    
    logger.info('Starting chatWithAI:', {
      messageCount: messages.length,
      estimatedTokenCount,
      lastMessage: messages[messages.length - 1]?.content.substring(0, 50) + '...'
    });

    const stream = await openai.chat.completions.create({
      model: "o3-mini-2025-01-31",
      messages: [
        { role: "system", content: SYSTEM_PROMPT },
        ...messages.map(msg => ({
          role: msg.role as "user" | "assistant" | "system",
          content: msg.content
        }))
      ],
      max_completion_tokens: 500,
      stream: true
    });

    logger.info('Stream created, starting to process chunks');

    let fullResponse = "";

    // Process each chunk from the stream
    for await (const chunk of stream) {
      const content = chunk.choices[0]?.delta?.content || "";

      if (content) {
        fullResponse += content;
        yield { response: content, streaming: true };
      }
    }

    logger.info('Stream completed:', {
      totalLength: fullResponse.length
    });

    // Send final confirmation
    yield { response: "", streaming: false };
  } catch (error) {
    logger.error("OpenAI API Error:", {
      error: error instanceof Error ? error.message : String(error),
      stack: error instanceof Error ? error.stack : undefined
    });
    throw error;
  }
}

/**
 * Calculate token usage for monitoring and optimization
 * @param text String to analyze for token count
 * @returns Estimated token count
 */
export function estimateTokenCount(text: string): number {
  // Conservative estimation of tokens
  // This is a rough approximation - about 4 characters per token for English text
  return Math.ceil(text.length / 4);
}

/**
 * Log token usage for analytics and monitoring
 * @param userId User ID
 * @param context Complete context string
 * @param response Complete response string
 */
export function logTokenUsage(userId: string | number, context: string, response: string): void {
  const contextTokens = estimateTokenCount(context);
  const responseTokens = estimateTokenCount(response);
  const totalTokens = contextTokens + responseTokens;
  
  logger.info('LLM Token Usage:', {
    userId,
    contextTokens,
    responseTokens,
    totalTokens,
    timestamp: new Date().toISOString()
  });
}